---
title: "カスタムデータモデル"
---
import { Callout } from 'nextra/components'

# カスタムデータモデル

<Callout type="warning">
これはレガシーインデクサーのドキュメントです。最新のインデクサースタックでカスタムプロセッサーを作成する方法については、[カスタムプロセッサー](../custom-processors.mdx)をご覧ください。
</Callout>

## 独自のデータモデルを定義する

Aptosレジャーデータ用のカスタムインデクサーを開発したい場合は、この方法を使用します。

<Callout type="info">
カスタムインデクサーを使用するのはいつですか？

現在、Aptosが提供するインデックスサービス（上記参照）は、以下のコアMoveモジュールをサポートしています：

- `0x1::coin`
- `0x3::token`
- `0x3::token_transfers`

他のMoveモジュールやコントラクトのインデックス付きデータベースが必要な場合は、カスタムインデクサーを開発する必要があります。
</Callout>

カスタムインデクサーの作成には以下の手順が含まれます。このドキュメントの冒頭にあるインデックス作成のブロック図を参照してください。

1. [Diesel](https://diesel.rs/)などのORMを使用して、新しいテーブルスキーマを定義します。このドキュメントではDieselを使用してカスタムインデックス作成手順（図の「ビジネスロジック」とデータクエリ）を説明します。
2. 新しいテーブルに基づいて新しいデータモデルを作成します（図の「ビジネスロジック」）。
3. 新しいトランザクションプロセッサーを作成するか、オプションで既存のプロセッサーに追加します。図では、この手順は新しいビジネスロジックに従ってレジャーデータベースを処理し、インデックス付きデータベースに書き込むことに対応します。
4. 新しいプロセッサーを統合します。既存のプロセッサーを再利用する場合はオプションです。

以下の詳細な説明では、コイン残高のインデックス作成とクエリの例を使用します。これは[`coin_processor.rs`](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/processors/coin_processor.rs)で確認できます。

### 1. 新しいテーブルスキーマを定義する

この例では、ORMとして[PostgreSQL](https://www.postgresql.org/)と[Diesel](https://diesel.rs/)を使用します。アップグレードのたびにデータベースをリセットすることなく、後方互換性のある変更を確実に行うために、[Diesel migrations](https://docs.rs/diesel_migrations/latest/diesel_migrations/)を使用してスキーマを管理します。そのため、他の作業を始める前に新しいDieselマイグレーションを生成することが非常に重要です。

`git clone https://github.com/aptos-labs/aptos-core.git`を実行してAptos-coreリポジトリをクローンし、`aptos-core/tree/main/crates/indexer`ディレクトリに`cd`で移動してください。その後、以下の手順に進みます。

a. 最初のステップは、新しいDieselマイグレーションを作成することです。これにより、[migrations](https://github.com/aptos-labs/aptos-core/tree/main/crates/indexer/migrations)の下に`up.sql`と`down.sql`を含む新しいフォルダが生成されます。

```bash filename="Terminal"
DATABASE_URL=postgres://postgres@localhost:5432/postgres diesel migration generate add_coin_tables
```

b. Create the necessary table schemas. This is just PostgreSQL code. In the code shown below, the `up.sql` will have the new changes and `down.sql` will revert those changes.

```sql filename="up.sql / down.sql"
-- up.sql
-- coin balances for each version
CREATE TABLE coin_balances (
  transaction_version BIGINT NOT NULL,
  owner_address VARCHAR(66) NOT NULL,
  -- Hash of the non-truncated coin type
  coin_type_hash VARCHAR(64) NOT NULL,
  -- creator_address::name::symbol<struct>
  coin_type VARCHAR(5000) NOT NULL,
  amount NUMERIC NOT NULL,
  transaction_timestamp TIMESTAMP NOT NULL,
  inserted_at TIMESTAMP NOT NULL DEFAULT NOW(),
  -- Constraints
  PRIMARY KEY (
    transaction_version,
    owner_address,
    coin_type_hash
  )
);
-- latest coin balances
CREATE TABLE current_coin_balances {...}
-- down.sql
DROP TABLE IF EXISTS coin_balances;
DROP TABLE IF EXISTS current_coin_balances;
```

See the [full source for `up.sql` and `down.sql`](https://github.com/aptos-labs/aptos-core/tree/main/crates/indexer/migrations/2022-10-04-073529_add_coin_tables).

c. Run the migration. We suggest running it multiple times with `redo` to ensure that both `up.sql` and `down.sql` are implemented correctly. This will also modify the [`schema.rs`](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/schema.rs) file.

```bash filename="Terminal"
DATABASE_URL=postgres://postgres@localhost:5432/postgres diesel migration run
DATABASE_URL=postgres://postgres@localhost:5432/postgres diesel migration redo
```

### 2. Create new data schemas

We now have to prepare the Rust data models that correspond to the Diesel schemas. In the case of coin balances, we will define `CoinBalance` and `CurrentCoinBalance` as below:

```rust filename="coin_balance.rs"
#[derive(Debug, Deserialize, FieldCount, Identifiable, Insertable, Serialize)]
#[diesel(primary_key(transaction_version, owner_address, coin_type))]
#[diesel(table_name = coin_balances)]
pub struct CoinBalance {
    pub transaction_version: i64,
    pub owner_address: String,
    pub coin_type_hash: String,
    pub coin_type: String,
    pub amount: BigDecimal,
    pub transaction_timestamp: chrono::NaiveDateTime,
}

#[derive(Debug, Deserialize, FieldCount, Identifiable, Insertable, Serialize)]
#[diesel(primary_key(owner_address, coin_type))]
#[diesel(table_name = current_coin_balances)]
pub struct CurrentCoinBalance {
    pub owner_address: String,
    pub coin_type_hash: String,
    pub coin_type: String,
    pub amount: BigDecimal,
    pub last_transaction_version: i64,
    pub last_transaction_timestamp: chrono::NaiveDateTime,
}
```

We will also need to specify the parsing logic, where the input is a portion of the transaction. In the case of coin balances, we can find all the details in `WriteSetChanges`, specifically where the write set change type is `write_resources`.

**Where to find the relevant data for parsing**: This requires a combination of understanding the Move module and the structure of the transaction. In the example of coin balance, the contract lives in [coin.move](https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/sources/coin.move), specifically the coin struct (search for `struct Coin`) that has a `value` field. We then look at an [example transaction](https://api.testnet.aptoslabs.com/v1/transactions/by_version/259518) where we find this exact structure in `write_resources`:

```bash filename="Terminal"
"changes": [
  {
    ...
    "data": {
      "type": "0x1::coin::CoinStore<0x1::aptos_coin::AptosCoin>",
      "data": {
        "coin": {
          "value": "49742"
      },
      ...
```

See the full code in [coin_balances.rs](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/models/coin_models/coin_balances.rs).

### 3. Create a new processor

Now that we have the data model and the parsing function, we need to call that parsing function and save the resulting model in our Postgres database. We do this by creating (or modifying) a `processor`. We have abstracted a lot already from that class, so the only function that should be implemented is `process_transactions` (there are a few more functions that should be copied, those should be obvious from the example).

The `process_transactions` function takes in a vector of transactions with a start and end version that are used for tracking purposes. The general flow should be:

- Loop through transactions in the vector.
- Aggregate relevant models. Sometimes deduping is required, e.g. in the case of `CurrentCoinBalance`.
- Insert the models into the database in a single Diesel transaction. This is important, to ensure that we do not have partial writes.
- Return status (error or success).

<Callout type="info">
See [coin_processor.rs](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/processors/coin_processor.rs) for a relatively straightforward example. You can search for `coin_balances` in the page for the specific code snippet related to coin balances.
</Callout>

**How to decide whether to create a new processor:** This is completely up to you. The benefit of creating a new processor is that you are starting from scratch, so you will have full control over exactly what gets written to the indexed database. The downside is that you will have to maintain a new fullnode, since there is a 1-to-1 mapping between a fullnode and the processor.

### 4. Integrate the new processor

This is the easiest step and involves just a few additions.

1. To start with, make sure to add the new processor in the Rust code files: [`mod.rs`](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/processors/mod.rs) and [`runtime.rs`](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/runtime.rs). See below:

[**mod.rs**](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/processors/mod.rs)

```rust filename="mod.rs"
pub enum Processor {
  CoinProcessor,
  ...
}
...
  COIN_PROCESSOR_NAME => Self::CoinProcessor,
```

[**runtime.rs**](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/runtime.rs)

```rust filename="runtime.rs"
Processor::CoinProcessor => Arc::new(CoinTransactionProcessor::new(conn_pool.clone())),
```

2. Create a `fullnode.yaml` with the correct configuration and test the custom indexer by starting a fullnode with this `fullnode.yaml`.

**fullnode.yaml**

```yaml filename="fullnode.yaml"
storage:
  enable_indexer: true
  storage_pruner_config:
    ledger_pruner_config:
      enable: false

indexer:
  enabled: true
  check_chain_id: true
  emit_every: 1000
  postgres_uri: "postgres://postgres@localhost:5432/postgres"
  processor: "coin_processor"
  fetch_tasks: 10
  processor_tasks: 10
```

Test by starting an Aptos fullnode by running the below command. You will see many logs in the terminal output, so use the `grep` filter to see only indexer log output, as shown below:

```bash filename="Terminal"
cargo run -p aptos-node --features "indexer" --release -- -f ./fullnode_coin.yaml | grep -E "_processor"
```

See the full instructions on how to start an indexer-enabled fullnode in [Indexer Fullnode](indexer-fullnode.mdx).
